#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ä½¿ç”¨OpenAIå®¢æˆ·ç«¯æµ‹è¯•LiteLLMæ¥å£
æµ‹è¯•é€šè¿‡OpenAI Pythonåº“è°ƒç”¨LiteLLMä»£ç†æœåŠ¡å™¨
"""

import os
import sys
import json
import asyncio
import logging
from datetime import datetime
import openai
# from platform_utils import (
#     get_platform_functions,
#     add_functions_to_request,
#     process_function_call,
#     add_platform_system_message,
#     get_platform_info
# )

# å¯¼å…¥é€šç”¨çš„æ—¥å¿—é…ç½®
try:
    from productAdapter.utils.logging_config import setup_logging, create_date_based_log_path
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„æœ¬åœ°ç‰ˆæœ¬
    def create_date_based_log_path(base_dir, filename):
        """åˆ›å»ºåŸºäºæ—¥æœŸçš„æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå¹´/æœˆ/æ—¥ç»“æ„ï¼‰"""
        now = datetime.now()
        year = str(now.year)
        month = f"{now.month:02d}"
        day = f"{now.day:02d}"
        
        date_dir = os.path.join(base_dir, year, month, day)
        try:
            os.makedirs(date_dir, exist_ok=True)
        except Exception as e:
            print(f"Error creating log directory {date_dir}: {str(e)}")
            date_dir = base_dir
        
        full_path = os.path.join(date_dir, filename)
        relative_path = os.path.join(year, month, day, filename)
        return full_path, relative_path
    
    def setup_logging(name="test_openai_client", level=None):
        """ç®€å•çš„æœ¬åœ°æ—¥å¿—é…ç½®å‡½æ•°"""
        # åˆ›å»ºlogsç›®å½•
        log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "logs")
        os.makedirs(log_dir, exist_ok=True)
        
        # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
        log_filename = f"{name}.log"
        log_file, relative_path = create_date_based_log_path(log_dir, log_filename)
        
        # é…ç½®æ ¹æ—¥å¿—å™¨
        logging.basicConfig(
            level=level or logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(sys.stdout),
                logging.FileHandler(log_file, mode='a', encoding='utf-8')
            ],
            force=True
        )
        
        # åˆ›å»ºä¸“ç”¨çš„æµ‹è¯•æ—¥å¿—å™¨
        logger = logging.getLogger(name)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°æ–‡ä»¶ï¼ˆå½“æ—¥é¦–æ¬¡è¿è¡Œï¼‰
        is_new_file = not os.path.exists(log_file) or os.path.getsize(log_file) == 0
        
        if is_new_file:
            logger.info("=" * 80)
            logger.info(f"ğŸ“… æµ‹è¯•æ—¥å¿—å¼€å§‹ - {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}")
            logger.info("=" * 80)
        else:
            logger.info("-" * 50)
            logger.info(f"ğŸ”„ æ–°æµ‹è¯•ä¼šè¯å¼€å§‹ - {datetime.now().strftime('%H:%M:%S')}")
            logger.info("-" * 50)
        
        logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {relative_path}")
        
        return logger

def get_test_logger():
    """
    è·å–æµ‹è¯•ä¸“ç”¨çš„æ—¥å¿—è®°å½•å™¨
    
    Returns:
        tuple: (logger, log_file_path)
    """
    # ä½¿ç”¨é€šç”¨çš„setup_loggingå‡½æ•°
    logger = setup_logging(name="test_openai_client", level=logging.INFO)
    
    # è®¡ç®—æ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "logs")
    log_filename = "test_openai_client.log"
    log_file, relative_path = create_date_based_log_path(log_dir, log_filename)
    
    return logger, log_file

# ç®€å•çš„ç¯å¢ƒå˜é‡è·å–å‡½æ•°
def get_env(key, default=None):
    return os.environ.get(key, default)

def get_env_int(key, default=None):
    """è·å–ç¯å¢ƒå˜é‡å¹¶è½¬æ¢ä¸ºæ•´æ•°"""
    value = get_env(key, default)
    return int(value) if value is not None else default

def test_openai_sync_client(logger=None):
    """
    æµ‹è¯•åŒæ­¥OpenAIå®¢æˆ·ç«¯è°ƒç”¨LiteLLM
    
    Args:
        logger: æ—¥å¿—è®°å½•å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°çš„
    """
    print("=== å¼€å§‹æµ‹è¯•åŒæ­¥OpenAIå®¢æˆ·ç«¯ ===")
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥loggerï¼Œåˆ™åˆ›å»ºæ–°çš„
    if logger is None:
        logger, _ = get_test_logger()
    
    # ä»ç¯å¢ƒå˜é‡è·å–LiteLLMä»£ç†çš„ä¸»æœºå’Œç«¯å£
    host = get_env("LITELLM_PROXY_HOST", "localhost")
    port = get_env_int("LITELLM_PROXY_PORT", 8080)
    base_url = f"http://{host}:{port}"
    
    message = f"LiteLLMä»£ç†åœ°å€: {base_url}"
    print(message)
    logger.info(message)
    
    # é…ç½®OpenAIå®¢æˆ·ç«¯
    client = openai.OpenAI(
        base_url=base_url,
        api_key="dummy-key"  # LiteLLMä»£ç†ä¸éœ€è¦çœŸå®APIå¯†é’¥
    )
    
    # ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹åç§°
    default_model = get_env("DEFAULT_MODEL", "my-custom-model")
    model_name = default_model
    message = f"ä½¿ç”¨æ¨¡å‹: {model_name}"
    print(message)
    logger.info(message)
    
    try:
        message = "å‘èµ·åŒæ­¥èŠå¤©è¯·æ±‚..."
        print(message)
        logger.info(message)
        
        # å‘èµ·èŠå¤©è¯·æ±‚
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´æ˜äº†çš„ä¸­æ–‡å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·ç”¨ä¸€å¥è¯å›ç­”ã€‚"}
            ],
            temperature=0.7,
            max_tokens=100
        )
        
        message = "âœ… åŒæ­¥è¯·æ±‚æˆåŠŸ"
        print(message)
        logger.info(message)
        
        # æ‰“å°è¯¦ç»†å“åº”ä¿¡æ¯
        print("\n=== åŒæ­¥OpenAIå®¢æˆ·ç«¯æµ‹è¯•ç»“æœ ===")
        print(f"æ¨¡å‹: {response.model}")
        print(f"å“åº”å†…å®¹: {response.choices[0].message.content}")
        print(f"å®ŒæˆåŸå› : {response.choices[0].finish_reason}")
        print(f"Tokenä½¿ç”¨æƒ…å†µ: {response.usage}")
        
        return True
        
    except Exception as e:
        error_msg = f"åŒæ­¥è¯·æ±‚å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        logger.error(error_msg)
        return False

def load_request_test_data():
    """
    ä»request_test.jsonæ–‡ä»¶åŠ è½½æµ‹è¯•æ•°æ®
    """
    try:
        request_test_file = os.path.join(os.path.dirname(__file__), "request_test.json")
        if os.path.exists(request_test_file):
            with open(request_test_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"âœ… æˆåŠŸåŠ è½½æµ‹è¯•æ•°æ®: {request_test_file}")
                return data.get("data", {})
        else:
            print(f"âš ï¸ æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {request_test_file}")
            return None
    except Exception as e:
        print(f"âŒ åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {str(e)}")
        return None

async def test_openai_async_client(logger=None):
    """
    æµ‹è¯•å¼‚æ­¥OpenAIå®¢æˆ·ç«¯è°ƒç”¨LiteLLMï¼Œä½¿ç”¨request_test.jsonä¸­çš„æ•°æ®
    
    Args:
        logger: æ—¥å¿—è®°å½•å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°çš„
    """
    # å¦‚æœæ²¡æœ‰ä¼ å…¥loggerï¼Œåˆ™åˆ›å»ºæ–°çš„
    if logger is None:
        logger, log_file = get_test_logger()
    else:
        log_file = None  # å¦‚æœä¼ å…¥äº†loggerï¼Œä¸éœ€è¦log_file
    
    # ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰“å°å„ç§çº§åˆ«çš„æ—¥å¿—
    logger.info("ğŸš€ å¼€å§‹æµ‹è¯•å¼‚æ­¥OpenAIå®¢æˆ·ç«¯")
    logger.info("ğŸ“ è¿™æ˜¯ä¿¡æ¯çº§åˆ«çš„æ—¥å¿—")
    logger.warning("âš ï¸ è¿™æ˜¯è­¦å‘Šçº§åˆ«çš„æ—¥å¿—")
    logger.error("âŒ è¿™æ˜¯é”™è¯¯çº§åˆ«çš„æ—¥å¿—")
    
    print("=== å¼€å§‹æµ‹è¯•å¼‚æ­¥OpenAIå®¢æˆ·ç«¯ ===")
    
    # ä»ç¯å¢ƒå˜é‡è·å–LiteLLMä»£ç†çš„ä¸»æœºå’Œç«¯å£
    host = get_env("LITELLM_PROXY_HOST", "localhost")
    port = get_env_int("LITELLM_PROXY_PORT", 8080)
    base_url = f"http://{host}:{port}"
    
    message = f"LiteLLMä»£ç†åœ°å€: {base_url}"
    print(message)
    logger.info(message)
    
    # é…ç½®å¼‚æ­¥OpenAIå®¢æˆ·ç«¯
    async_client = openai.AsyncOpenAI(
        base_url=base_url,
        api_key="dummy-key"  # LiteLLMä»£ç†ä¸éœ€è¦çœŸå®APIå¯†é’¥
    )
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = load_request_test_data()
    logger.info(f"åŠ è½½æµ‹è¯•æ•°æ®: {test_data}ï¼Œlog_file: {log_file}") 
    if test_data is None:
        message = "âŒ æ— æ³•åŠ è½½æµ‹è¯•æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°"
        print(message)
        logger.warning(message)
        # ä½¿ç”¨é»˜è®¤å‚æ•°
        model_name = get_env("DEFAULT_MODEL", "my-custom-model")
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”æŠ€æœ¯é—®é¢˜ã€‚"},
            {"role": "user", "content": "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹LiteLLMæ˜¯ä»€ä¹ˆï¼Ÿ"}
        ]
        request_params = {
            "model": model_name,
            "messages": messages,
            "temperature": 0.7,
            "max_tokens": 150
        }
    else:
        message = "âœ… ä½¿ç”¨request_test.jsonä¸­çš„æµ‹è¯•æ•°æ®"
        print(message)
        logger.info(message)
        # ä½¿ç”¨æµ‹è¯•æ•°æ®ï¼Œä½†å°†æ¨¡å‹åç§°æ›¿æ¢ä¸ºå¯ç”¨çš„æ¨¡å‹
        model_name = get_env("DEFAULT_MODEL", "my-custom-model")
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": model_name,  # ä½¿ç”¨å¯ç”¨çš„æ¨¡å‹
            "messages": test_data.get("messages", []),
            "temperature": test_data.get("temperature", 0.7),
            "max_tokens": test_data.get("max_tokens", 1000)
        }
        
        # å¦‚æœæœ‰response_formatï¼Œä¹ŸåŒ…å«è¿›å»ï¼ˆä½†å¯èƒ½éœ€è¦ç®€åŒ–ä»¥é€‚é…å½“å‰æ¨¡å‹ï¼‰
        if "response_format" in test_data:
            message = "ğŸ“ åŒ…å«structured outputæ ¼å¼"
            print(message)
            logger.info(message)
            # ä½¿ç”¨getæ–¹æ³•å®‰å…¨è·å–response_format
            request_params["response_format"] = test_data.get("response_format")
        
        # å¦‚æœæœ‰streamå‚æ•°
        if "stream" in test_data:
            message = f"ğŸ”„ æµå¼æ¨¡å¼: {test_data['stream']}"
            print(message)
            logger.info(message)
            request_params["stream"] = test_data.get("stream", False)  # ä½¿ç”¨test_dataä¸­çš„streamå€¼
    
    # message = f"ä½¿ç”¨æ¨¡å‹: {request_params['model']}"
    # print(message)
    # logger.info(message)
    # message = f"æ¶ˆæ¯æ•°é‡: {len(request_params['messages'])}"
    # print(message)
    # logger.info(message)
    
    try:
        message = "å‘èµ·å¼‚æ­¥èŠå¤©è¯·æ±‚..."
        print(message)
        logger.info(message)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæµå¼æ¨¡å¼
        if request_params.get("stream", False):
            message = "ğŸ”„ ä½¿ç”¨å¼‚æ­¥æµå¼æ¨¡å¼..."
            print(message)
            logger.info(message)
            
            try:
                # å‘èµ·å¼‚æ­¥æµå¼èŠå¤©è¯·æ±‚
                stream = await async_client.chat.completions.create(**request_params)
                
                print("\n=== å¼‚æ­¥æµå¼OpenAIå®¢æˆ·ç«¯æµ‹è¯•ç»“æœ ===")
                print("æµå¼å“åº”å†…å®¹:")
                
                collected_content = ""
                try:
                    # ä½¿ç”¨æ­£ç¡®çš„å¼‚æ­¥æµå¼å¤„ç†æ–¹å¼
                    async for chunk in stream:
                        if hasattr(chunk.choices[0], 'delta') and chunk.choices[0].delta.content is not None:
                            content = chunk.choices[0].delta.content
                            collected_content += content
                            print(content, end="", flush=True)
                        elif hasattr(chunk.choices[0], 'message') and chunk.choices[0].message.content is not None:
                            content = chunk.choices[0].message.content
                            collected_content += content
                            print(content, end="", flush=True)
                except TypeError as stream_error:
                    # å¤„ç† 'coroutine' object is not an iterator é”™è¯¯
                    error_msg = f"å¼‚æ­¥æµå¼å¤„ç†å¤±è´¥ï¼ˆLiteLLMå…¼å®¹æ€§é—®é¢˜ï¼‰: {str(stream_error)}"
                    logger.warning(f"âš ï¸ {error_msg}")
                    print(f"\nâš ï¸ {error_msg}")
                    
                    # å°è¯•ä½¿ç”¨éæµå¼æ¨¡å¼ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
                    message = "ğŸ”„ å°è¯•ä½¿ç”¨éæµå¼æ¨¡å¼ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ..."
                    print(message)
                    logger.info(message)
                    
                    # åˆ›å»ºéæµå¼è¯·æ±‚å‚æ•°
                    non_stream_params = request_params.copy()
                    non_stream_params["stream"] = False
                    
                    # å‘èµ·éæµå¼è¯·æ±‚
                    response = await async_client.chat.completions.create(**non_stream_params)
                    
                    if hasattr(response.choices[0], 'message') and response.choices[0].message.content:
                        content = response.choices[0].message.content
                        collected_content = content
                        print(f"ğŸ“ å¤‡é€‰æ–¹æ¡ˆå“åº”å†…å®¹: {content}")
                        logger.info(f"ğŸ“ å¤‡é€‰æ–¹æ¡ˆå“åº”å†…å®¹: {content}")
                    else:
                        error_msg = "å¤‡é€‰æ–¹æ¡ˆä¹Ÿå¤±è´¥äº†"
                        logger.error(f"âŒ {error_msg}")
                        print(f"âŒ {error_msg}")
                        return False
                        
                except Exception as stream_error:
                    error_msg = f"æµå¼å¤„ç†å¤±è´¥: {str(stream_error)}"
                    logger.error(f"âŒ {error_msg}")
                    print(f"\nâŒ {error_msg}")
                    return False
                
                print("\n")  # æ¢è¡Œ
                message = "âœ… å¼‚æ­¥æµå¼è¯·æ±‚æˆåŠŸ"
                print(message)
                logger.info(message)
                
                # æ‰“å°æ”¶é›†åˆ°çš„å®Œæ•´å†…å®¹
                print(f"ğŸ“ å®Œæ•´å“åº”å†…å®¹: {collected_content}")
                logger.info(f"ğŸ“ å®Œæ•´å“åº”å†…å®¹: {collected_content}")
                
            except Exception as e:
                error_msg = f"å¼‚æ­¥æµå¼è¯·æ±‚å¤±è´¥: {str(e)}"
                logger.error(f"âŒ {error_msg}")
                print(f"âŒ {error_msg}")
                
                # å°è¯•ä½¿ç”¨éæµå¼æ¨¡å¼ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
                message = "ğŸ”„ å°è¯•ä½¿ç”¨éæµå¼æ¨¡å¼ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ..."
                print(message)
                logger.info(message)
                
                try:
                    # åˆ›å»ºéæµå¼è¯·æ±‚å‚æ•°
                    non_stream_params = request_params.copy()
                    non_stream_params["stream"] = False
                    
                    # å‘èµ·éæµå¼è¯·æ±‚
                    response = await async_client.chat.completions.create(**non_stream_params)
                    
                    if hasattr(response.choices[0], 'message') and response.choices[0].message.content:
                        content = response.choices[0].message.content
                        print(f"ğŸ“ å¤‡é€‰æ–¹æ¡ˆå“åº”å†…å®¹: {content}")
                        logger.info(f"ğŸ“ å¤‡é€‰æ–¹æ¡ˆå“åº”å†…å®¹: {content}")
                        return True
                    else:
                        error_msg = "å¤‡é€‰æ–¹æ¡ˆä¹Ÿå¤±è´¥äº†"
                        logger.error(f"âŒ {error_msg}")
                        print(f"âŒ {error_msg}")
                        return False
                        
                except Exception as backup_error:
                    error_msg = f"å¤‡é€‰æ–¹æ¡ˆä¹Ÿå¤±è´¥äº†: {str(backup_error)}"
                    logger.error(f"âŒ {error_msg}")
                    print(f"âŒ {error_msg}")
                    return False
        
        else:
            # å‘èµ·æ™®é€šå¼‚æ­¥èŠå¤©è¯·æ±‚
            response = await async_client.chat.completions.create(**request_params)
            
            message = "âœ… å¼‚æ­¥è¯·æ±‚æˆåŠŸ"
            print(message)
            logger.info(message)
            
            # æ‰“å°è¯¦ç»†å“åº”ä¿¡æ¯
            print("\n=== å¼‚æ­¥OpenAIå®¢æˆ·ç«¯æµ‹è¯•ç»“æœ ===")
            print(f"æ¨¡å‹: {response.model}")
            print(f"å“åº”å†…å®¹: {response.choices[0].message.content}")
            print(f"å®ŒæˆåŸå› : {response.choices[0].finish_reason}")
            print(f"Tokenä½¿ç”¨æƒ…å†µ: {response.usage}")
        
        return True
        
    except Exception as e:
        error_msg = f"å¼‚æ­¥è¯·æ±‚å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        logger.error(error_msg)
        return False

async def test_openai_structured_output(logger=None):
    """
    æµ‹è¯•å¼‚æ­¥OpenAIå®¢æˆ·ç«¯çš„structured outputåŠŸèƒ½ï¼Œä½¿ç”¨request_test.jsonä¸­çš„å®Œæ•´æ•°æ®
    
    Args:
        logger: æ—¥å¿—è®°å½•å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°çš„
    """
    print("=== å¼€å§‹æµ‹è¯•OpenAI structured output ===")
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥loggerï¼Œåˆ™åˆ›å»ºæ–°çš„
    if logger is None:
        logger, _ = get_test_logger()
    
    # ä»ç¯å¢ƒå˜é‡è·å–LiteLLMä»£ç†çš„ä¸»æœºå’Œç«¯å£
    host = get_env("LITELLM_PROXY_HOST", "localhost")
    port = get_env_int("LITELLM_PROXY_PORT", 8080)
    base_url = f"http://{host}:{port}"
    
    message = f"LiteLLMä»£ç†åœ°å€: {base_url}"
    print(message)
    logger.info(message)
    
    # é…ç½®å¼‚æ­¥OpenAIå®¢æˆ·ç«¯
    async_client = openai.AsyncOpenAI(
        base_url=base_url,
        api_key="dummy-key"  # LiteLLMä»£ç†ä¸éœ€è¦çœŸå®APIå¯†é’¥
    )
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = load_request_test_data()
    if test_data is None:
        message = "âŒ æ— æ³•åŠ è½½æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡structured outputæµ‹è¯•"
        print(message)
        logger.warning(message)
        return False
    
    message = "âœ… ä½¿ç”¨request_test.jsonä¸­çš„å®Œæ•´æµ‹è¯•æ•°æ®"
    print(message)
    logger.info(message)
    
    # ä½¿ç”¨æµ‹è¯•æ•°æ®ä¸­çš„æ‰€æœ‰å‚æ•°ï¼Œä½†æ›¿æ¢æ¨¡å‹åç§°
    model_name = get_env("DEFAULT_MODEL", "my-custom-model")
    
    # æ„å»ºå®Œæ•´çš„è¯·æ±‚å‚æ•°
    request_params = {
        "model": model_name,  # ä½¿ç”¨å¯ç”¨çš„æ¨¡å‹
        "messages": test_data.get("messages", []),
        # "temperature": test_data.get("temperature", 0.7),
        # "max_tokens": test_data.get("max_tokens", 1000)
    }
    
    # ä½¿ç”¨å¹³å°å·¥å…·æ¨¡å—æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ˜ç¡®å‘Šè¯‰ LLM å½“å‰å¹³å°æ˜¯ PPT
    # request_params["messages"] = add_platform_system_message(request_params["messages"], platform="PPT")
    # logger.info("âœ… ä½¿ç”¨å¹³å°å·¥å…·æ¨¡å—æ·»åŠ äº†ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ˜ç¡®å‘Šè¯‰ LLM å½“å‰å¹³å°æ˜¯ PPT")
    
    # ä½¿ç”¨å¹³å°å·¥å…·æ¨¡å—æ·»åŠ  functions å‚æ•°ï¼Œæ˜ç¡®æŒ‡å®šå¹³å°ä¸º PPT
    # request_params = add_functions_to_request(request_params, platform="PPT")
    # logger.info("âœ… ä½¿ç”¨å¹³å°å·¥å…·æ¨¡å—æ·»åŠ äº† functions å‚æ•°ï¼Œæ˜ç¡®æŒ‡å®šå¹³å°ä¸º PPT")

    # logger.info(f"ğŸ” æ·»åŠ äº† functions å‚æ•°: {json.dumps(request_params.get('functions'), ensure_ascii=False, indent=2)}")

    logger.info(f"ğŸ” å®Œæ•´response_format: {json.dumps(test_data.get('response_format'), ensure_ascii=False, indent=2)}")

    # åŒ…å«response_formatï¼ˆå¦‚æœæ”¯æŒçš„è¯ï¼‰
    if "response_format" in test_data:
        message = "ğŸ“ æµ‹è¯•structured outputæ ¼å¼"
        print(message)
        logger.info(message)
        try:
            request_params["response_format"] = test_data.get("response_format")
        except Exception as e:
            message = f"âš ï¸ ä¸æ”¯æŒresponse_formatï¼Œå°†å¿½ç•¥: {str(e)}"
            print(message)
            logger.warning(message)
    
    # message = f"ä½¿ç”¨æ¨¡å‹: {request_params['model']}"
    # print(message)
    # logger.info(message)
    # message = f"æ¶ˆæ¯æ•°é‡: {len(request_params['messages'])}"
    # print(message)
    # logger.info(message)
    # message = f"åŒ…å«response_format: {'response_format' in request_params}"
    # print(message)
    # logger.info(message)
    
    # è°ƒè¯•ï¼šæ‰“å°å®Œæ•´çš„request_params
    message = f"ğŸ” å®Œæ•´request_params keys: {list(request_params.keys())}"
    print(message)
    logger.info(f"ğŸ” å®Œæ•´request_params: {json.dumps(request_params, ensure_ascii=False, indent=2)}")
    # if 'response_format' in request_params:
    #     message = f"ğŸ” response_formatå†…å®¹: {json.dumps(request_params['response_format'], ensure_ascii=False, indent=2)[:200]}..."
    #     print(message)
    #     logger.info(message)
    
    try:
        message = "å‘èµ·structured outputè¯·æ±‚..."
        print(message)
        logger.info(message)
        
        # å‘èµ·å¼‚æ­¥èŠå¤©è¯·æ±‚
        response = await async_client.chat.completions.create(**request_params)
        
        print("âœ… structured outputè¯·æ±‚æˆåŠŸ")
        print(f"response: {json.dumps(response.model_dump(), ensure_ascii=False, indent=2)}")
        
        # ä½¿ç”¨å¹³å°å·¥å…·æ¨¡å—å¤„ç†å‡½æ•°è°ƒç”¨å“åº”
        # function_call = response.choices[0].message.function_call
        # function_args = process_function_call(function_call, logger)
        
        return True
        
    except Exception as e:
        error_msg = f"structured outputè¯·æ±‚å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        return False

def test_openai_stream_client(logger=None):
    """
    æµ‹è¯•æµå¼OpenAIå®¢æˆ·ç«¯è°ƒç”¨LiteLLM
    
    Args:
        logger: æ—¥å¿—è®°å½•å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°çš„
    """
    print("=== å¼€å§‹æµ‹è¯•æµå¼OpenAIå®¢æˆ·ç«¯ ===")
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥loggerï¼Œåˆ™åˆ›å»ºæ–°çš„
    if logger is None:
        logger, _ = get_test_logger()
    
    # ä»ç¯å¢ƒå˜é‡è·å–LiteLLMä»£ç†çš„ä¸»æœºå’Œç«¯å£
    host = get_env("LITELLM_PROXY_HOST", "localhost")
    port = get_env_int("LITELLM_PROXY_PORT", 8080)
    base_url = f"http://{host}:{port}"
    
    message = f"LiteLLMä»£ç†åœ°å€: {base_url}"
    print(message)
    logger.info(message)
    
    # é…ç½®OpenAIå®¢æˆ·ç«¯
    client = openai.OpenAI(
        base_url=base_url,
        api_key="dummy-key"  # LiteLLMä»£ç†ä¸éœ€è¦çœŸå®APIå¯†é’¥
    )
    
    # ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹åç§°
    default_model = get_env("DEFAULT_MODEL", "my-custom-model")
    model_name = default_model
    message = f"ä½¿ç”¨æ¨¡å‹: {model_name}"
    print(message)
    logger.info(message)
    
    try:
        message = "å‘èµ·æµå¼èŠå¤©è¯·æ±‚..."
        print(message)
        logger.info(message)
        
        print("\n=== æµå¼OpenAIå®¢æˆ·ç«¯æµ‹è¯•ç»“æœ ===")
        print("æµå¼å“åº”å†…å®¹:")
        
        # å‘èµ·æµå¼èŠå¤©è¯·æ±‚
        stream = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåˆ›æ„å†™ä½œåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡åˆ›ä½œã€‚"},
                {"role": "user", "content": "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—ï¼Œ4è¡Œå³å¯ã€‚"}
            ],
            temperature=0.8,
            max_tokens=200,
            stream=True
        )
        
        collected_content = ""
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                collected_content += content
                print(content, end="", flush=True)
        
        print("\n")  # æ¢è¡Œ
        message = "âœ… æµå¼è¯·æ±‚æˆåŠŸ"
        print(message)
        logger.info(message)
        
        return True
        
    except Exception as e:
        error_msg = f"æµå¼è¯·æ±‚å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        logger.error(error_msg)
        return False

def test_openai_models_list(logger=None):
    """
    æµ‹è¯•OpenAIå®¢æˆ·ç«¯è·å–æ¨¡å‹åˆ—è¡¨åŠŸèƒ½
    
    Args:
        logger: æ—¥å¿—è®°å½•å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°çš„
    """
    print("=== å¼€å§‹æµ‹è¯•æ¨¡å‹åˆ—è¡¨è·å– ===")
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥loggerï¼Œåˆ™åˆ›å»ºæ–°çš„
    if logger is None:
        logger, _ = get_test_logger()
    
    # ä»ç¯å¢ƒå˜é‡è·å–LiteLLMä»£ç†çš„ä¸»æœºå’Œç«¯å£
    host = get_env("LITELLM_PROXY_HOST", "localhost")
    port = get_env_int("LITELLM_PROXY_PORT", 8080)
    base_url = f"http://{host}:{port}"
    
    # åŒæ—¶æ‰“å°å’Œè®°å½•åˆ°æ—¥å¿—
    message = f"LiteLLMä»£ç†åœ°å€: {base_url}"
    print(message)
    logger.info(message)
    
    # é…ç½®OpenAIå®¢æˆ·ç«¯
    client = openai.OpenAI(
        base_url=base_url,
        api_key="dummy-key"  # LiteLLMä»£ç†ä¸éœ€è¦çœŸå®APIå¯†é’¥
    )
    
    try:
        message = "è·å–æ¨¡å‹åˆ—è¡¨..."
        print(message)
        logger.info(message)
        
        # è·å–æ¨¡å‹åˆ—è¡¨
        models = client.models.list()
        
        message = "âœ… æ¨¡å‹åˆ—è¡¨è·å–æˆåŠŸ"
        print(message)
        logger.info(message)
        
        # æ‰“å°è¯¦ç»†æ¨¡å‹ä¿¡æ¯
        print("\n=== å¯ç”¨æ¨¡å‹åˆ—è¡¨ ===")
        print(f"æ¨¡å‹æ€»æ•°: {len(models.data)}")
        
        for i, model in enumerate(models.data, 1):
            print(f"{i}. æ¨¡å‹ID: {model.id}")
            print(f"   ç±»å‹: {model.object}")
            print(f"   æ‹¥æœ‰è€…: {model.owned_by}")
            print(f"   åˆ›å»ºæ—¶é—´: {model.created}")
            if hasattr(model, 'root'):
                print(f"   æ ¹æ¨¡å‹: {model.root}")
            print()
        
        # éªŒè¯æ˜¯å¦åŒ…å«é¢„æœŸçš„æ¨¡å‹
        model_ids = [model.id for model in models.data]
        expected_models = ["my-custom-model", "business-presentation-model"]
        
        for expected in expected_models:
            if expected in model_ids:
                print(f"âœ… å‘ç°é¢„æœŸæ¨¡å‹: {expected}")
            else:
                print(f"âš ï¸ æœªå‘ç°é¢„æœŸæ¨¡å‹: {expected}")
        
        return True
        
    except Exception as e:
        error_msg = f"æ¨¡å‹åˆ—è¡¨è·å–å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        logger.error(error_msg)
        return False

def test_openai_multiple_models(logger=None):
    """
    æµ‹è¯•å¤šä¸ªæ¨¡å‹è°ƒç”¨
    
    Args:
        logger: æ—¥å¿—è®°å½•å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°çš„
    """
    print("=== å¼€å§‹æµ‹è¯•å¤šä¸ªæ¨¡å‹è°ƒç”¨ ===")
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥loggerï¼Œåˆ™åˆ›å»ºæ–°çš„
    if logger is None:
        logger, _ = get_test_logger()
    
    # ä»ç¯å¢ƒå˜é‡è·å–LiteLLMä»£ç†çš„ä¸»æœºå’Œç«¯å£
    host = get_env("LITELLM_PROXY_HOST", "localhost")
    port = get_env_int("LITELLM_PROXY_PORT", 8080)
    base_url = f"http://{host}:{port}"
    
    message = f"LiteLLMä»£ç†åœ°å€: {base_url}"
    print(message)
    logger.info(message)
    
    # é…ç½®OpenAIå®¢æˆ·ç«¯
    client = openai.OpenAI(
        base_url=base_url,
        api_key="dummy-key"
    )
    
    # æµ‹è¯•ä¸åŒçš„æ¨¡å‹åç§°
    models_to_test = [
        "my-custom-model",
        get_env("DEFAULT_MODEL", "my-custom-model")
    ]
    
    print("\n=== å¤šæ¨¡å‹æµ‹è¯•ç»“æœ ===")
    successful_models = []
    failed_models = []
    
    for model in models_to_test:
        try:
            message = f"æµ‹è¯•æ¨¡å‹: {model}"
            print(message)
            logger.info(message)
            
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": "è¯·è¯´ 'ä½ å¥½'"}
                ],
                temperature=0.5,
                max_tokens=50
            )
            
            content = response.choices[0].message.content
            message = f"âœ… æ¨¡å‹ '{model}': {content}"
            print(message)
            logger.info(message)
            successful_models.append(model)
            
        except Exception as e:
            error_msg = f"æ¨¡å‹ '{model}' å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            logger.error(error_msg)
            failed_models.append(model)
    
    message = f"æˆåŠŸçš„æ¨¡å‹: {successful_models}"
    print(message)
    logger.info(message)
    message = f"å¤±è´¥çš„æ¨¡å‹: {failed_models}"
    print(message)
    logger.info(message)
    
    return len(successful_models) > 0

async def run_all_tests(logger=None):
    """
    è¿è¡Œæ‰€æœ‰æµ‹è¯•
    
    Args:
        logger: æ—¥å¿—è®°å½•å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°çš„
    """
    print("ğŸš€ å¼€å§‹è¿è¡ŒOpenAIå®¢æˆ·ç«¯æµ‹è¯•å¥—ä»¶")
    
    # å¦‚æœæ²¡æœ‰ä¼ å…¥loggerï¼Œåˆ™åˆ›å»ºæ–°çš„
    if logger is None:
        logger, _ = get_test_logger()
    
    # è®°å½•ç¯å¢ƒå˜é‡ä¿¡æ¯
    print(f"ç¯å¢ƒå˜é‡: LITELLM_PROXY_HOST={get_env('LITELLM_PROXY_HOST', 'æœªè®¾ç½®')}, "
               f"LITELLM_PROXY_PORT={get_env('LITELLM_PROXY_PORT', 'æœªè®¾ç½®')}, "
               f"DEFAULT_MODEL={get_env('DEFAULT_MODEL', 'æœªè®¾ç½®')}")
    
    results = {}
    
    # æµ‹è¯•1: åŒæ­¥å®¢æˆ·ç«¯
    results['sync'] = test_openai_sync_client(logger=logger)
    
    # æµ‹è¯•2: å¼‚æ­¥å®¢æˆ·ç«¯
    results['async'] = await test_openai_async_client(logger=logger)
    
    # æµ‹è¯•3: æµå¼å®¢æˆ·ç«¯
    results['stream'] = test_openai_stream_client(logger=logger)
    
    # æµ‹è¯•4: æ¨¡å‹åˆ—è¡¨æµ‹è¯•
    results['models_list'] = test_openai_models_list(logger=logger)
    
    # æµ‹è¯•5: å¤šæ¨¡å‹æµ‹è¯•
    results['multiple_models'] = test_openai_multiple_models(logger=logger)
    
    # æµ‹è¯•6: Structured Outputæµ‹è¯•
    results['structured'] = await test_openai_structured_output(logger=logger)
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*50)
    print("ğŸ æµ‹è¯•æ€»ç»“")
    print("="*50)
    
    total_tests = len(results)
    passed_tests = sum(1 for result in results.values() if result)
    
    for test_name, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name.ljust(20)}: {status}")
    
    print(f"\næ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
    print(f"å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}")
    print(f"é€šè¿‡ç‡: {passed_tests/total_tests*100:.1f}%")
    
    return passed_tests == total_tests

def parse_arguments():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    """
    import argparse
    parser = argparse.ArgumentParser(description="ä½¿ç”¨OpenAIå®¢æˆ·ç«¯æµ‹è¯•LiteLLMæ¥å£")
    parser.add_argument("--test", type=str, choices=['sync', 'async', 'stream', 'models_list', 'models', 'structured', 'all'],
                        default='async', help="æŒ‡å®šè¦è¿è¡Œçš„æµ‹è¯•ç±»å‹")
    
    return parser.parse_args()

async def main():
    """
    ä¸»å‡½æ•°
    """
    # è®¾ç½®æ—¥å¿—è®°å½•
    logger, log_file = get_test_logger()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # ä½¿ç”¨configç›®å½•ä¸‹çš„.envæ–‡ä»¶
    config_env_file = os.path.join(os.path.dirname(os.path.dirname(__file__)), "config", ".env")
    if os.path.exists(config_env_file):
        message = f"âœ… ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_env_file}"
        print(message)
        logger.info(message)
    else:
        message = f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_env_file}"
        print(message)
        logger.warning(message)
    
    # æ ¹æ®å‚æ•°è¿è¡ŒæŒ‡å®šæµ‹è¯•
    if args.test == 'sync':
        success = test_openai_sync_client(logger=logger)
    elif args.test == 'async':
        success = await test_openai_async_client(logger=logger)
    elif args.test == 'stream':
        success = test_openai_stream_client(logger=logger)
    elif args.test == 'models_list':
        success = test_openai_models_list(logger=logger)
    elif args.test == 'models':
        success = test_openai_multiple_models(logger=logger)
    elif args.test == 'structured':
        success = await test_openai_structured_output(logger=logger)
    else:  # all
        success = await run_all_tests(logger=logger)
    
    # è®°å½•æµ‹è¯•ä¼šè¯ç»“æŸ
    logger.info("-" * 50)
    status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
    logger.info(f"ğŸ æµ‹è¯•ä¼šè¯ç»“æŸ - {status} - {datetime.now().strftime('%H:%M:%S')}")
    logger.info("-" * 50)
    logger.info("")  # ç©ºè¡Œåˆ†éš”
    
    # é€€å‡ºç 
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())