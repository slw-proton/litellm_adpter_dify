services:
  dify-openai:
    build:
      context: .
      dockerfile: Dockerfile
    image: litellm-adapter:latest
    container_name: dify-openai
    working_dir: /app
    ports:
      - "18002:8002"   # Business API
      - "18080:8080"   # LiteLLM Proxy
    env_file:
      - .env.ppt 
    volumes:
      - ./logs/docker:/var/log/trae-litellm
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://0.0.0.0:8002/health"]
      interval: 10s
      timeout: 5s
      retries: 10
    restart: unless-stopped
    networks:
      - idolverse-network

networks:
  idolverse-network:
    external: true


