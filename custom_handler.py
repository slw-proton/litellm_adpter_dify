#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
è‡ªå®šä¹‰LiteLLMå¤„ç†å™¨
æ ¹æ®LiteLLMå®˜æ–¹æ–‡æ¡£åˆ›å»º
"""

import os
import sys
import json
import time
import uuid
import logging
import requests
from typing import Dict, List, Any, Optional, Iterator, AsyncIterator

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

# å¯¼å…¥LiteLLMç›¸å…³æ¨¡å—
import litellm
from litellm import CustomLLM, completion, get_llm_provider
from litellm.types.utils import GenericStreamingChunk, ModelResponse

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class MyCustomLLM(CustomLLM):
    """
    è‡ªå®šä¹‰LLMå¤„ç†å™¨
    ç»§æ‰¿è‡ªLiteLLMçš„CustomLLMåŸºç±»
    """
    
    def __init__(self):
        super().__init__()
        # ä½¿ç”¨æ¨¡æ‹ŸSSEæœåŠ¡å™¨è¿›è¡Œæµ‹è¯•
        self.api_base = "http://localhost:8002/api/process"
        print("[custom_handler] MyCustomLLMåˆå§‹åŒ–å®Œæˆ - ä½¿ç”¨æ¨¡æ‹ŸSSEæœåŠ¡å™¨")
    
    def _extract_response_format(self, kwargs: dict, key: str = "response_format") -> tuple[Optional[Dict[str, Any]], str]:
        """
        ä»kwargsä¸­æå–æŒ‡å®šå‚æ•°å¹¶ç¡®å®šå“åº”ç±»å‹
        
        Args:
            kwargs: åŒ…å«optional_paramså’ŒæŒ‡å®šå‚æ•°çš„å‚æ•°å­—å…¸
            key: è¦æå–çš„å‚æ•°åç§°ï¼Œé»˜è®¤ä¸º "response_format"
            
        Returns:
            tuple: (extracted_value, response_type)
                - extracted_value: æå–çš„å‚æ•°å€¼æˆ–None
                - response_type: å“åº”ç±»å‹å­—ç¬¦ä¸² ("text" æˆ– "json")
        
        Examples:
            # æå–response_formatå‚æ•°
            response_format, response_type = self._extract_response_format(kwargs, "response_format")
            
            # æå–temperatureå‚æ•°
            temperature, _ = self._extract_response_format(kwargs, "temperature")
        """
        if not isinstance(kwargs, dict):
            print(f"[custom_handler] âš ï¸ kwargsä¸æ˜¯å­—å…¸ç±»å‹: {type(kwargs)}")
            return None, "text"
        
        # å®‰å…¨åœ°æ£€æŸ¥optional_paramsä¸­çš„æŒ‡å®šå‚æ•°
        optional_params = kwargs.get('optional_params', {})
        if optional_params and not isinstance(optional_params, dict):
            print(f"[custom_handler] âš ï¸ optional_paramsä¸æ˜¯å­—å…¸ç±»å‹: {type(optional_params)}")
            optional_params = {}
        
        print(f"[custom_handler] optional_params keys: {list(optional_params.keys()) if optional_params else 'None'}")
        
        # ä¼˜å…ˆä»optional_paramsä¸­è·å–æŒ‡å®šå‚æ•°ï¼Œå¦‚æœæ²¡æœ‰å†ä»kwargsä¸­è·å–
        extracted_value = None
        if optional_params and key in optional_params:
            extracted_value = optional_params[key]
            print(f"[custom_handler] âœ… ä»optional_paramsæ£€æµ‹åˆ°{key}: {extracted_value.get('type', 'unknown') if isinstance(extracted_value, dict) else extracted_value}")
            if extracted_value and isinstance(extracted_value, dict):
                print(f"[custom_handler] optional_params.{key}è¯¦æƒ…: {json.dumps(extracted_value, ensure_ascii=False, indent=2)}")
        elif kwargs.get(key) is not None:
            extracted_value = kwargs.get(key)
            print(f"[custom_handler] âœ… ä»kwargsæ£€æµ‹åˆ°{key}: {extracted_value.get('type', 'unknown') if isinstance(extracted_value, dict) else extracted_value}")
            if isinstance(extracted_value, dict):
                print(f"[custom_handler] {key}è¯¦æƒ…: {json.dumps(extracted_value, ensure_ascii=False, indent=2)}")
        else:
            print(f"[custom_handler] âš ï¸ æœªæ£€æµ‹åˆ°{key}å‚æ•°ï¼ˆkwargs.{key}={kwargs.get(key)}, optional_params.{key}={optional_params.get(key) if optional_params else 'N/A'}ï¼‰")
        
        # ç¡®å®šå“åº”ç±»å‹ï¼ˆä»…å¯¹response_formatå‚æ•°ï¼‰
        response_type = "text"
        if key == "response_format" and extracted_value:
            if isinstance(extracted_value, dict):
                if extracted_value.get("type") == "json_schema":
                    response_type = "json"
                    print(f"[custom_handler] è®¾ç½®å“åº”ç±»å‹ä¸ºJSONï¼ˆstructured outputï¼‰")
                elif extracted_value.get("type") == "json_object":
                    response_type = "json"
                    print(f"[custom_handler] è®¾ç½®å“åº”ç±»å‹ä¸ºJSON object")
        
        return extracted_value, response_type
    
    def completion(self, *args, **kwargs) -> litellm.ModelResponse:
        """
        åŒæ­¥å®Œæˆæ–¹æ³•
        æ ¹æ®å®˜æ–¹æ–‡æ¡£å®ç°
        """
        try:
            # ä»kwargsä¸­æå–å‚æ•°
            model = kwargs.get("model", "business-api")
            messages = kwargs.get("messages", [])
            max_tokens = kwargs.get("max_tokens", 100)
            temperature = kwargs.get("temperature", 0.7)
            print(f'[custom_handler] messages: {messages}')
            # ç¡®ä¿messagesæ˜¯æ•°ç»„æ ¼å¼
            if not isinstance(messages, list):
                print(f"[custom_handler] è­¦å‘Šï¼šmessagesä¸æ˜¯æ•°ç»„æ ¼å¼ï¼Œç±»å‹ä¸º{type(messages)}ï¼Œè½¬æ¢ä¸ºæ•°ç»„")
                if isinstance(messages, str):
                    messages = [{"role": "user", "content": messages}]
                elif messages is None:
                    messages = []
                else:
                    messages = [{"role": "user", "content": str(messages)}]
            
            print(f"[custom_handler] å¤„ç†completionè¯·æ±‚: model={model}, messages={len(messages)}æ¡æ¶ˆæ¯")
            print(f"[custom_handler] å®Œæ•´kwargs keys: {list(kwargs.keys())}")
            
            # æå–response_formatå¹¶ç¡®å®šå“åº”ç±»å‹
            response_format, response_type = self._extract_response_format(kwargs, "response_format")
            messages.append({"role": "response_format", "content": response_format})
            stream = self._extract_response_format(kwargs, "stream")
            # æ„å»ºä¸šåŠ¡APIè¯·æ±‚
            business_request = {
                "query": messages,  # å…¨é‡è½¬å‘å®Œæ•´çš„messagesæ•°ç»„
                "model_info": {
                    "name": model
                },
                "response_type": response_type,
                "stream": stream,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            print(f"[custom_handler] å‘é€åˆ°ä¸šåŠ¡APIçš„è¯·æ±‚: {json.dumps(business_request, ensure_ascii=False, indent=2)}")
            # å‘é€è¯·æ±‚åˆ°ä¸šåŠ¡API
            response = requests.post(
                self.api_base,
                json=business_request,
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            
            if response.status_code == 200:
                business_response = response.json()
                print(f"[custom_handler] ä¸šåŠ¡APIå“åº”: {json.dumps(business_response, ensure_ascii=False, indent=2)}")
                
                # ä¿®å¤ï¼šæ­£ç¡®æå–contentå­—æ®µ
                content = business_response.get("content", "Hello from custom LLM!")
                if isinstance(content, dict) and "message" in content:
                    # æå–messageå­—æ®µä¸­çš„JSONå­—ç¬¦ä¸²
                    mock_response = content["message"]
                    print(f"[custom_handler] æå–åˆ°JSONå†…å®¹: {mock_response[:100]}...")
                else:
                    # å¦‚æœä¸æ˜¯é¢„æœŸæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨content
                    mock_response = content if isinstance(content, str) else str(content)
                    print(f"[custom_handler] ä½¿ç”¨åŸå§‹å†…å®¹: {mock_response}")
                
                # ç¡®ä¿mock_responseä¸ä¸ºç©º
                if not mock_response or mock_response.strip() == "":
                    mock_response = "Hello from custom LLM! (ä¸šåŠ¡APIè¿”å›ç©ºå†…å®¹)"
                    print(f"[custom_handler] ä¸šåŠ¡APIè¿”å›ç©ºå†…å®¹ï¼Œä½¿ç”¨é»˜è®¤å“åº”: {mock_response}")
                
                return litellm.completion(
                    model="gpt-3.5-turbo",  # ä½¿ç”¨ä¸€ä¸ªå·²çŸ¥çš„æ¨¡å‹æ ¼å¼
                    messages=messages,  # ä½¿ç”¨å®Œæ•´çš„messagesæ•°ç»„
                    mock_response=mock_response,
                    api_key="dummy-key",  # æ·»åŠ api_keyå‚æ•°
                )
            else:
                print(f"[custom_handler] ä¸šåŠ¡APIé”™è¯¯: {response.status_code} - {response.text}")
                # è¿”å›é”™è¯¯å“åº”
                return litellm.completion(
                    model="gpt-3.5-turbo",
                    messages=messages,  # ä½¿ç”¨å®Œæ•´çš„messagesæ•°ç»„
                    mock_response="æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚",
                    api_key="dummy-key",  # æ·»åŠ api_keyå‚æ•°
                )
                
        except Exception as e:
            print(f"[custom_handler] å¤„ç†completionè¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
            # ç¡®ä¿messagesæ˜¯æ•°ç»„æ ¼å¼
            if 'messages' in locals():
                if not isinstance(messages, list):
                    if isinstance(messages, str):
                        messages = [{"role": "user", "content": messages}]
                    elif messages is None:
                        messages = []
                    else:
                        messages = [{"role": "user", "content": str(messages)}]
            else:
                messages = []
            
            # è¿”å›é”™è¯¯å“åº”
            return litellm.completion(
                model="gpt-3.5-turbo",
                messages=messages,  # ä½¿ç”¨å®Œæ•´çš„messagesæ•°ç»„
                mock_response="æŠ±æ­‰ï¼Œå¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ã€‚",
                api_key="dummy-key",  # æ·»åŠ api_keyå‚æ•°
            )

    async def acompletion(self, *args, **kwargs) -> litellm.ModelResponse:
        """
        å¼‚æ­¥å®Œæˆæ–¹æ³•
        æ ¹æ®å®˜æ–¹æ–‡æ¡£å®ç°
        """
        try:
            # ä»kwargsä¸­æå–å‚æ•°
            model = kwargs.get("model", "business-api")
            messages = kwargs.get("messages", [])
            max_tokens = kwargs.get("max_tokens", 100)
            temperature = kwargs.get("temperature", 0.7)
            stream = kwargs.get("stream", False)
            
            print(f'[custom_handler] async messages: {messages}')
            # ç¡®ä¿messagesæ˜¯æ•°ç»„æ ¼å¼
            if not isinstance(messages, list):
                print(f"[custom_handler] è­¦å‘Šï¼šmessagesä¸æ˜¯æ•°ç»„æ ¼å¼ï¼Œç±»å‹ä¸º{type(messages)}ï¼Œè½¬æ¢ä¸ºæ•°ç»„")
                if isinstance(messages, str):
                    messages = [{"role": "user", "content": messages}]
                elif messages is None:
                    messages = []
                else:
                    messages = [{"role": "user", "content": str(messages)}]
            
            print(f"[custom_handler] å¤„ç†async completionè¯·æ±‚: model={model}, messages={len(messages)}æ¡æ¶ˆæ¯, stream={stream}")
            
            # æå–response_formatå¹¶ç¡®å®šå“åº”ç±»å‹
            response_format, response_type = self._extract_response_format(kwargs, "response_format")
            messages.append({"role": "response_format", "content": response_format})
            
            # æ„å»ºä¸šåŠ¡APIè¯·æ±‚
            business_request = {
                "query": messages,  # å…¨é‡è½¬å‘å®Œæ•´çš„messagesæ•°ç»„
                "model_info": {
                    "name": model
                },
                "response_type": response_type,
                "stream": stream,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            print(f"[custom_handler] å‘é€åˆ°ä¸šåŠ¡APIçš„å¼‚æ­¥è¯·æ±‚: {json.dumps(business_request, ensure_ascii=False, indent=2)}")
            
            # ä½¿ç”¨aiohttpè¿›è¡Œå¼‚æ­¥è¯·æ±‚
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.api_base,
                    json=business_request,
                    headers={"Content-Type": "application/json"},
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        business_response = await response.json()
                        print(f"[custom_handler] ä¸šåŠ¡APIå¼‚æ­¥å“åº”: {json.dumps(business_response, ensure_ascii=False, indent=2)}")
                        
                        # ä¿®å¤ï¼šæ­£ç¡®æå–contentå­—æ®µ
                        content = business_response.get("content", "Hello from custom LLM!")
                        if isinstance(content, dict) and "message" in content:
                            # æå–messageå­—æ®µä¸­çš„JSONå­—ç¬¦ä¸²
                            mock_response = content["message"]
                            print(f"[custom_handler] æå–åˆ°JSONå†…å®¹: {mock_response[:100]}...")
                        else:
                            # å¦‚æœä¸æ˜¯é¢„æœŸæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨content
                            mock_response = content if isinstance(content, str) else str(content)
                            print(f"[custom_handler] ä½¿ç”¨åŸå§‹å†…å®¹: {mock_response}")
                        
                        # ç¡®ä¿mock_responseä¸ä¸ºç©º
                        if not mock_response or mock_response.strip() == "":
                            mock_response = "Hello from custom LLM! (ä¸šåŠ¡APIè¿”å›ç©ºå†…å®¹)"
                            print(f"[custom_handler] ä¸šåŠ¡APIè¿”å›ç©ºå†…å®¹ï¼Œä½¿ç”¨é»˜è®¤å“åº”: {mock_response}")
                        
                        return litellm.completion(
                            model="gpt-3.5-turbo",  # ä½¿ç”¨ä¸€ä¸ªå·²çŸ¥çš„æ¨¡å‹æ ¼å¼
                            messages=messages,  # ä½¿ç”¨å®Œæ•´çš„messagesæ•°ç»„
                            mock_response=mock_response,
                            api_key="dummy-key",  # æ·»åŠ api_keyå‚æ•°
                        )
                    else:
                        error_text = await response.text()
                        print(f"[custom_handler] ä¸šåŠ¡APIé”™è¯¯: {response.status} - {error_text}")
                        # è¿”å›é”™è¯¯å“åº”
                        return litellm.completion(
                            model="gpt-3.5-turbo",
                            messages=messages,  # ä½¿ç”¨å®Œæ•´çš„messagesæ•°ç»„
                            mock_response="æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚",
                            api_key="dummy-key",  # æ·»åŠ api_keyå‚æ•°
                        )
                        
        except Exception as e:
            print(f"[custom_handler] å¤„ç†async completionè¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
            # ç¡®ä¿messagesæ˜¯æ•°ç»„æ ¼å¼
            if 'messages' in locals():
                if not isinstance(messages, list):
                    if isinstance(messages, str):
                        messages = [{"role": "user", "content": messages}]
                    elif messages is None:
                        messages = []
                    else:
                        messages = [{"role": "user", "content": str(messages)}]
            else:
                messages = []
            
            # è¿”å›é”™è¯¯å“åº”
            return litellm.completion(
                model="gpt-3.5-turbo",
                messages=messages,  # ä½¿ç”¨å®Œæ•´çš„messagesæ•°ç»„
                mock_response="æŠ±æ­‰ï¼Œå¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ã€‚",
                api_key="dummy-key",  # æ·»åŠ api_keyå‚æ•°
            )

    def streaming(self, *args, **kwargs) -> Iterator[GenericStreamingChunk]:
        """
        åŒæ­¥æµå¼å¤„ç†æ–¹æ³•
        æ ¹æ®å®˜æ–¹æ–‡æ¡£å®ç°
        """
        try:
            # ä»kwargsä¸­æå–å‚æ•°
            model = kwargs.get("model", "business-api")
            messages = kwargs.get("messages", [])
            max_tokens = kwargs.get("max_tokens", 100)
            temperature = kwargs.get("temperature", 0.7)
            
            print(f'[custom_handler] streaming messages: {messages}')
            # ç¡®ä¿messagesæ˜¯æ•°ç»„æ ¼å¼
            if not isinstance(messages, list):
                if isinstance(messages, str):
                    messages = [{"role": "user", "content": messages}]
                elif messages is None:
                    messages = []
                else:
                    messages = [{"role": "user", "content": str(messages)}]
            
            print(f"[custom_handler] å¤„ç†streamingè¯·æ±‚: model={model}, messages={len(messages)}æ¡æ¶ˆæ¯")
            
            # æå–response_formatå¹¶ç¡®å®šå“åº”ç±»å‹
            response_format, response_type = self._extract_response_format(kwargs, "response_format")
            messages.append({"role": "response_format", "content": response_format})
            
            # æ„å»ºä¸šåŠ¡APIè¯·æ±‚
            business_request = {
                "query": messages,  # å…¨é‡è½¬å‘å®Œæ•´çš„messagesæ•°ç»„
                "model_info": {
                    "name": model
                },
                "response_type": response_type,
                "stream": True,  # å¼ºåˆ¶è®¾ç½®ä¸ºæµå¼
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            print(f"[custom_handler] å‘é€åˆ°ä¸šåŠ¡APIçš„åŒæ­¥æµå¼è¯·æ±‚: {json.dumps(business_request, ensure_ascii=False, indent=2)}")
            
            # ä½¿ç”¨requestsè¿›è¡ŒåŒæ­¥è¯·æ±‚
            import requests
            
            try:
                response = requests.post(
                    self.api_base,
                    json=business_request,
                    headers={"Content-Type": "application/json"},
                    timeout=60,
                    stream=True
                )
                
                if response.status_code == 200:
                    # å¤„ç†æµå¼å“åº” - é€ä¸ªè¿”å›æ¯ä¸ªSSEæ•°æ®å—
                    chunk_count = 0
                    for line in response.iter_lines(decode_unicode=True):
                        if line:
                            chunk_count += 1
                            print(f"[custom_handler] ğŸ”„ STREAMING ç¬¬{chunk_count}ä¸ªæ•°æ®å—: {line[:100]}...")
                            
                            # è§£æSSEæ•°æ®
                            if line.startswith('data: '):
                                try:
                                    # ç§»é™¤ "data: " å‰ç¼€
                                    data_content = line[6:].strip()  # ç§»é™¤ "data: " å‰ç¼€å¹¶å»é™¤ç©ºç™½
                                    
                                    if data_content == '[DONE]':
                                        # æµç»“æŸ
                                        print(f"[custom_handler] ğŸ STREAMING æµç»“æŸä¿¡å·")
                                        final_chunk: GenericStreamingChunk = {
                                            "finish_reason": "stop",
                                            "index": 0,
                                            "is_finished": True,
                                            "text": "",
                                            "tool_use": None,
                                            "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                                        }
                                        yield final_chunk
                                        break
                                    else:
                                        # å°è¯•è§£æå¤–å±‚JSONæ•°æ®ï¼ˆDifyçš„åµŒå¥—æ ¼å¼ï¼‰
                                        try:
                                            outer_data = json.loads(data_content)
                                            
                                            # æ£€æŸ¥æ˜¯å¦æ˜¯Difyçš„åµŒå¥—æ ¼å¼
                                            if isinstance(outer_data, dict) and "type" in outer_data and "chunk" in outer_data:
                                                # è¿™æ˜¯Difyçš„åµŒå¥—æ ¼å¼ï¼Œéœ€è¦è§£æå†…å±‚çš„chunk
                                                chunk_content = outer_data.get("chunk", "")
                                                if chunk_content:
                                                    # è§£æå†…å±‚çš„chunkå†…å®¹
                                                    try:
                                                        inner_data = json.loads(chunk_content)
                                                        text_content = self._extract_text_from_sse_data(inner_data)
                                                        if text_content == "__WORKFLOW_FINISHED__":
                                                            print(f"[custom_handler] ğŸ STREAMING å·¥ä½œæµå®Œæˆ(ä¿®å¤å)")
                                                            final_chunk: GenericStreamingChunk = {
                                                                "finish_reason": "stop",
                                                                "index": 0,
                                                                "is_finished": True,
                                                                "text": "",
                                                                "tool_use": None,
                                                                "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                                                            }
                                                            yield final_chunk
                                                            return
                                                        elif text_content:
                                                            print(f"[custom_handler] ğŸ“¤ STREAMING Yielding text_chunkå†…å®¹(ä¿®å¤å): {text_content[:50]}...")
                                                            generic_streaming_chunk: GenericStreamingChunk = {
                                                                "finish_reason": None,
                                                                "index": 0,
                                                                "is_finished": False,
                                                                "text": text_content,
                                                                "tool_use": None,
                                                                "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                                                            }
                                                            yield generic_streaming_chunk
                                                        else:
                                                            print(f"[custom_handler] âš ï¸ STREAMING text_chunkå†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                                                            
                                                    except json.JSONDecodeError as inner_e:
                                                        # å†…å±‚JSONè§£æå¤±è´¥ï¼Œå¯èƒ½æ˜¯éƒ¨åˆ†æ•°æ®æˆ–å•å¼•å·æ ¼å¼
                                                        print(f"[custom_handler] âš ï¸ å†…å±‚JSONè§£æå¤±è´¥: {str(inner_e)}, chunkå†…å®¹: {chunk_content[:100]}...")
                                                        # å°è¯•å¤„ç†å•å¼•å·æ ¼å¼
                                                        try:
                                                            # æ›¿æ¢å•å¼•å·ä¸ºåŒå¼•å·
                                                            chunk_content_fixed = chunk_content.replace("'", '"')
                                                            inner_data = json.loads(chunk_content_fixed)
                                                            text_content = self._extract_text_from_sse_data(inner_data)
                                                            if text_content:
                                                                print(f"[custom_handler] ğŸ“¤ STREAMING Yielding text_chunkå†…å®¹(ä¿®å¤å): {text_content[:50]}...")
                                                                generic_streaming_chunk: GenericStreamingChunk = {
                                                                    "finish_reason": None,
                                                                    "index": 0,
                                                                    "is_finished": False,
                                                                    "text": text_content,
                                                                    "tool_use": None,
                                                                    "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                                                                }
                                                                yield generic_streaming_chunk
                                                        except:
                                                            continue
                                            else:
                                                # ä¸æ˜¯Difyçš„åµŒå¥—æ ¼å¼ï¼ŒæŒ‰åŸæ¥çš„æ–¹å¼å¤„ç†
                                                text_content = self._extract_text_from_sse_data(outer_data)
                                                if text_content == "__WORKFLOW_FINISHED__":
                                                    print(f"[custom_handler] ğŸ STREAMING å·¥ä½œæµå®Œæˆ(ä¿®å¤åå¤–å±‚)")
                                                    final_chunk: GenericStreamingChunk = {
                                                        "finish_reason": "stop",
                                                        "index": 0,
                                                        "is_finished": True,
                                                        "text": "",
                                                        "tool_use": None,
                                                        "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                                                    }
                                                    yield final_chunk
                                                    return
                                                elif text_content:
                                                    print(f"[custom_handler] ğŸ“¤ STREAMING Yieldingå†…å®¹(ä¿®å¤å): {text_content[:50]}...")
                                                    generic_streaming_chunk: GenericStreamingChunk = {
                                                        "finish_reason": None,
                                                        "index": 0,
                                                        "is_finished": False,
                                                        "text": text_content,
                                                        "tool_use": None,
                                                        "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                                                    }
                                                    yield generic_streaming_chunk
                                                else:
                                                    print(f"[custom_handler] âš ï¸ STREAMING ç›´æ¥å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                                                    
                                        except json.JSONDecodeError as outer_e:
                                            # å¤–å±‚JSONè§£æå¤±è´¥ï¼Œå¯èƒ½æ˜¯éƒ¨åˆ†æ•°æ®æˆ–å…¶ä»–æ ¼å¼
                                            print(f"[custom_handler] âš ï¸ å¤–å±‚JSONè§£æå¤±è´¥: {str(outer_e)}, åŸå§‹å†…å®¹: {data_content[:100]}...")
                                            # å°è¯•å¤„ç†å•å¼•å·æ ¼å¼
                                            try:
                                                # æ›¿æ¢å•å¼•å·ä¸ºåŒå¼•å·
                                                data_content_fixed = data_content.replace("'", '"')
                                                outer_data = json.loads(data_content_fixed)
                                                text_content = self._extract_text_from_sse_data(outer_data)
                                                if text_content:
                                                    print(f"[custom_handler] ğŸ“¤ STREAMING Yieldingå†…å®¹(ä¿®å¤å): {text_content[:50]}...")
                                                    generic_streaming_chunk: GenericStreamingChunk = {
                                                        "finish_reason": None,
                                                        "index": 0,
                                                        "is_finished": False,
                                                        "text": text_content,
                                                        "tool_use": None,
                                                        "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                                                    }
                                                    yield generic_streaming_chunk
                                            except:
                                                continue
                                            
                                except Exception as e:
                                    # å¦‚æœæ•´ä½“è§£æå¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†
                                    print(f"[custom_handler] âš ï¸ SSEè§£æå¼‚å¸¸: {str(e)}, è¡Œå†…å®¹: {line[:100]}...")
                                    continue
                    
                    # ç¡®ä¿å‘é€å®Œæˆä¿¡å·
                    print(f"[custom_handler] ğŸ STREAMING å‘é€æœ€ç»ˆå®Œæˆä¿¡å·ï¼Œæ€»å…±å¤„ç†äº†{chunk_count}ä¸ªæ•°æ®å—")
                    final_chunk: GenericStreamingChunk = {
                        "finish_reason": "stop",
                        "index": 0,
                        "is_finished": True,
                        "text": "",
                        "tool_use": None,
                        "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                    }
                    yield final_chunk
                else:
                    error_text = response.text
                    print(f"[custom_handler] ä¸šåŠ¡APIè¿”å›é”™è¯¯: {response.status_code} - {error_text}")
                    # å‘é€é”™è¯¯å—
                    error_chunk: GenericStreamingChunk = {
                        "finish_reason": "stop",
                        "index": 0,
                        "is_finished": True,
                        "text": f"ä¸šåŠ¡APIé”™è¯¯: {response.status_code} - {error_text}",
                        "tool_use": None,
                        "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                    }
                    yield error_chunk
                    
            except Exception as e:
                error_msg = f"è¯·æ±‚ä¸šåŠ¡APIå¤±è´¥: {str(e)}"
                print(f"[custom_handler] {error_msg}")
                # å‘é€é”™è¯¯å—
                error_chunk: GenericStreamingChunk = {
                    "finish_reason": "stop",
                    "index": 0,
                    "is_finished": True,
                    "text": f"è¯·æ±‚å¤±è´¥: {error_msg}",
                    "tool_use": None,
                    "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                }
                yield error_chunk
                
        except Exception as e:
            error_msg = f"åŒæ­¥æµå¼å¤„ç†å¤±è´¥: {str(e)}"
            print(f"âŒ [custom_handler] {error_msg}")
            logger.error(f"âŒ [custom_handler] {error_msg}")
            # å‘é€é”™è¯¯å—
            error_chunk: GenericStreamingChunk = {
                "finish_reason": "stop",
                "index": 0,
                "is_finished": True,
                "text": f"å¤„ç†å¤±è´¥: {error_msg}",
                "tool_use": None,
                "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
            }
            yield error_chunk

    async def astreaming(self, *args, **kwargs) -> AsyncIterator[GenericStreamingChunk]:
        """
        å¼‚æ­¥æµå¼å¤„ç†æ–¹æ³•
        ä½¿ç”¨çœŸæ­£çš„å¼‚æ­¥æµå¼è¯»å–ï¼Œä¿æŒå³æ—¶å¤„ç†
        """
        try:
            # ä»kwargsä¸­æå–å‚æ•°
            model = kwargs.get("model", "business-api")
            messages = kwargs.get("messages", [])
            max_tokens = kwargs.get("max_tokens", 100)
            temperature = kwargs.get("temperature", 0.7)
            
            print(f'[custom_handler] async streaming messages: {messages}')
            # ç¡®ä¿messagesæ˜¯æ•°ç»„æ ¼å¼
            if not isinstance(messages, list):
                if isinstance(messages, str):
                    messages = [{"role": "user", "content": messages}]
                elif messages is None:
                    messages = []
                else:
                    messages = [{"role": "user", "content": str(messages)}]
            
            print(f"[custom_handler] å¤„ç†async streamingè¯·æ±‚: model={model}, messages={len(messages)}æ¡æ¶ˆæ¯")
            
            # æå–response_formatå¹¶ç¡®å®šå“åº”ç±»å‹
            response_format, response_type = self._extract_response_format(kwargs, "response_format")
            messages.append({"role": "response_format", "content": response_format})
            
            # æ„å»ºä¸šåŠ¡APIè¯·æ±‚
            business_request = {
                "query": messages,  # å…¨é‡è½¬å‘å®Œæ•´çš„messagesæ•°ç»„
                "model_info": {
                    "name": model
                },
                "response_type": response_type,
                "stream": True,  # å¼ºåˆ¶è®¾ç½®ä¸ºæµå¼
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            print(f"[custom_handler] å‘é€åˆ°ä¸šåŠ¡APIçš„å¼‚æ­¥æµå¼è¯·æ±‚: {json.dumps(business_request, ensure_ascii=False, indent=2)}")
            
            # ä½¿ç”¨aiohttpè¿›è¡Œå¼‚æ­¥è¯·æ±‚
            import aiohttp
            
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        self.api_base,
                        json=business_request,
                        headers={"Content-Type": "application/json"},
                        timeout=aiohttp.ClientTimeout(total=60)
                    ) as response:
                        
                        if response.status == 200:
                            # å¤„ç†æµå¼å“åº” - ä¿æŒå³æ—¶å¤„ç†
                            chunk_count = 0
                            
                            print(f"[custom_handler] ğŸ”„ å¼€å§‹å¼‚æ­¥æµå¼è¯»å–å“åº”...")
                            
                            # ä½¿ç”¨è¾ƒå°çš„chunk sizeæé«˜å®æ—¶æ€§ï¼Œä½†ä¿æŒå³æ—¶å¤„ç†
                            async for chunk in response.content.iter_chunked(256):
                                if chunk:
                                    chunk_count += 1
                                    # è§£ç å­—èŠ‚ä¸ºå­—ç¬¦ä¸²
                                    chunk_str = chunk.decode('utf-8', errors='ignore')
                                    print(f"[custom_handler] ğŸ”„ ASYNC_STREAMING ç¬¬{chunk_count}ä¸ªç½‘ç»œå— (å¤§å°: {len(chunk)}å­—èŠ‚): {chunk_str[:200]}...")
                                    
                                    # æŒ‰è¡Œåˆ†å‰²å¹¶ç«‹å³å¤„ç†æ¯ä¸ªSSEè¡Œ - ä¿æŒå³æ—¶å¤„ç†
                                    lines = chunk_str.split('\n')
                                    for line in lines:
                                        line_str = line.strip()
                                        if line_str and line_str.startswith('data: '):
                                            try:
                                                # ç§»é™¤ "data: " å‰ç¼€
                                                data_content = line_str[6:].strip()  # ç§»é™¤ "data: " å‰ç¼€å¹¶å»é™¤ç©ºç™½
                                                
                                                if data_content == '[DONE]':
                                                    print(f"[custom_handler] ğŸ ASYNC_STREAMING æµç»“æŸä¿¡å·")
                                                    final_chunk: GenericStreamingChunk = {
                                                        "finish_reason": "stop",
                                                        "index": 0,
                                                        "is_finished": True,
                                                        "text": "",
                                                        "tool_use": None,
                                                        "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                                                    }
                                                    yield final_chunk
                                                    return
                                                
                                                # å°è¯•è§£æå¤–å±‚JSONæ•°æ®ï¼ˆDifyçš„åµŒå¥—æ ¼å¼ï¼‰
                                                try:
                                                    outer_data = json.loads(data_content)
                                                    
                                                    # æ£€æŸ¥æ˜¯å¦æ˜¯Difyçš„åµŒå¥—æ ¼å¼
                                                    if isinstance(outer_data, dict) and "type" in outer_data and "chunk" in outer_data:
                                                        # è¿™æ˜¯Difyçš„åµŒå¥—æ ¼å¼ï¼Œéœ€è¦è§£æå†…å±‚çš„chunk
                                                        chunk_content = outer_data.get("chunk", "")
                                                        if chunk_content:
                                                            # è§£æå†…å±‚çš„chunkå†…å®¹
                                                            try:
                                                                inner_data = json.loads(chunk_content)
                                                                text_content = self._extract_text_from_sse_data(inner_data)
                                                                if text_content == "__WORKFLOW_FINISHED__":
                                                                    print(f"[custom_handler] ğŸ ASYNC_STREAMING å·¥ä½œæµå®Œæˆ")
                                                                    final_chunk: GenericStreamingChunk = {
                                                                        "finish_reason": "stop",
                                                                        "index": 0,
                                                                        "is_finished": True,
                                                                        "text": "",
                                                                        "tool_use": None,
                                                                        "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                                                                    }
                                                                    yield final_chunk
                                                                    return
                                                                elif text_content:
                                                                    print(f"[custom_handler] ğŸ“¤ ASYNC_STREAMING Yielding text_chunkå†…å®¹: {text_content[:50]}...")
                                                                    generic_streaming_chunk: GenericStreamingChunk = {
                                                                        "finish_reason": None,
                                                                        "index": 0,
                                                                        "is_finished": False,
                                                                        "text": text_content,
                                                                        "tool_use": None,
                                                                        "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                                                                    }
                                                                    yield generic_streaming_chunk
                                                                else:
                                                                    print(f"[custom_handler] âš ï¸ ASYNC_STREAMING text_chunkå†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                                                                    
                                                            except json.JSONDecodeError as inner_e:
                                                                # å†…å±‚JSONè§£æå¤±è´¥ï¼Œå¯èƒ½æ˜¯éƒ¨åˆ†æ•°æ®æˆ–å•å¼•å·æ ¼å¼
                                                                print(f"[custom_handler] âš ï¸ å†…å±‚JSONè§£æå¤±è´¥: {str(inner_e)}, chunkå†…å®¹: {chunk_content[:100]}...")
                                                                # å°è¯•å¤„ç†å•å¼•å·æ ¼å¼
                                                                try:
                                                                    # æ›¿æ¢å•å¼•å·ä¸ºåŒå¼•å·
                                                                    chunk_content_fixed = chunk_content.replace("'", '"')
                                                                    inner_data = json.loads(chunk_content_fixed)
                                                                    text_content = self._extract_text_from_sse_data(inner_data)
                                                                    if text_content:
                                                                        print(f"[custom_handler] ğŸ“¤ ASYNC_STREAMING Yielding text_chunkå†…å®¹(ä¿®å¤å): {text_content[:50]}...")
                                                                        generic_streaming_chunk: GenericStreamingChunk = {
                                                                            "finish_reason": None,
                                                                            "index": 0,
                                                                            "is_finished": False,
                                                                            "text": text_content,
                                                                            "tool_use": None,
                                                                            "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                                                                        }
                                                                        yield generic_streaming_chunk
                                                                except:
                                                                    continue
                                                    else:
                                                        # ä¸æ˜¯Difyçš„åµŒå¥—æ ¼å¼ï¼ŒæŒ‰åŸæ¥çš„æ–¹å¼å¤„ç†
                                                        text_content = self._extract_text_from_sse_data(outer_data)
                                                        if text_content == "__WORKFLOW_FINISHED__":
                                                            print(f"[custom_handler] ğŸ ASYNC_STREAMING å·¥ä½œæµå®Œæˆ(ç›´æ¥æ ¼å¼)")
                                                            final_chunk: GenericStreamingChunk = {
                                                                "finish_reason": "stop",
                                                                "index": 0,
                                                                "is_finished": True,
                                                                "text": "",
                                                                "tool_use": None,
                                                                "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                                                            }
                                                            yield final_chunk
                                                            return
                                                        elif text_content:
                                                            print(f"[custom_handler] ğŸ“¤ ASYNC_STREAMING Yieldingç›´æ¥å†…å®¹: {text_content[:50]}...")
                                                            generic_streaming_chunk: GenericStreamingChunk = {
                                                                "finish_reason": None,
                                                                "index": 0,
                                                                "is_finished": False,
                                                                "text": text_content,
                                                                "tool_use": None,
                                                                "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                                                            }
                                                            yield generic_streaming_chunk
                                                        else:
                                                            print(f"[custom_handler] âš ï¸ ASYNC_STREAMING ç›´æ¥å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                                                            
                                                except json.JSONDecodeError as outer_e:
                                                    # å¤–å±‚JSONè§£æå¤±è´¥ï¼Œå¯èƒ½æ˜¯éƒ¨åˆ†æ•°æ®æˆ–å…¶ä»–æ ¼å¼
                                                    print(f"[custom_handler] âš ï¸ å¤–å±‚JSONè§£æå¤±è´¥: {str(outer_e)}, åŸå§‹å†…å®¹: {data_content[:100]}...")
                                                    # å°è¯•å¤„ç†å•å¼•å·æ ¼å¼
                                                    try:
                                                        # æ›¿æ¢å•å¼•å·ä¸ºåŒå¼•å·
                                                        data_content_fixed = data_content.replace("'", '"')
                                                        outer_data = json.loads(data_content_fixed)
                                                        text_content = self._extract_text_from_sse_data(outer_data)
                                                        if text_content:
                                                            print(f"[custom_handler] ğŸ“¤ ASYNC_STREAMING Yieldingå†…å®¹(ä¿®å¤å): {text_content[:50]}...")
                                                            generic_streaming_chunk: GenericStreamingChunk = {
                                                                "finish_reason": None,
                                                                "index": 0,
                                                                "is_finished": False,
                                                                "text": text_content,
                                                                "tool_use": None,
                                                                "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                                                            }
                                                            yield generic_streaming_chunk
                                                    except:
                                                        continue
                                            
                                            except Exception as e:
                                                # å¦‚æœæ•´ä½“è§£æå¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†
                                                print(f"[custom_handler] âš ï¸ SSEè§£æå¼‚å¸¸: {str(e)}, è¡Œå†…å®¹: {line_str[:100]}...")
                                                continue
                            
                            # ç¡®ä¿å‘é€å®Œæˆä¿¡å·
                            print(f"[custom_handler] ğŸ ASYNC_STREAMING å‘é€æœ€ç»ˆå®Œæˆä¿¡å·ï¼Œæ€»å…±å¤„ç†äº†{chunk_count}ä¸ªæ•°æ®å—")
                            final_chunk: GenericStreamingChunk = {
                                "finish_reason": "stop",
                                "index": 0,
                                "is_finished": True,
                                "text": "",
                                "tool_use": None,
                                "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                            }
                            yield final_chunk
                        else:
                            error_text = await response.text()
                            print(f"[custom_handler] ä¸šåŠ¡APIè¿”å›é”™è¯¯: {response.status} - {error_text}")
                            # å‘é€é”™è¯¯å—
                            error_chunk: GenericStreamingChunk = {
                                "finish_reason": "stop",
                                "index": 0,
                                "is_finished": True,
                                "text": f"ä¸šåŠ¡APIé”™è¯¯: {response.status} - {error_text}",
                                "tool_use": None,
                                "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                            }
                            yield error_chunk
                            
            except Exception as e:
                error_msg = f"è¯·æ±‚ä¸šåŠ¡APIå¤±è´¥: {str(e)}"
                print(f"[custom_handler] {error_msg}")
                # å‘é€é”™è¯¯å—
                error_chunk: GenericStreamingChunk = {
                    "finish_reason": "stop",
                    "index": 0,
                    "is_finished": True,
                    "text": f"è¯·æ±‚å¤±è´¥: {error_msg}",
                    "tool_use": None,
                    "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
                }
                yield error_chunk
                
        except Exception as e:
            error_msg = f"å¼‚æ­¥æµå¼å¤„ç†å¤±è´¥: {str(e)}"
            print(f"âŒ [custom_handler] {error_msg}")
            logger.error(f"âŒ [custom_handler] {error_msg}")
            # å‘é€é”™è¯¯å—
            error_chunk: GenericStreamingChunk = {
                "finish_reason": "stop",
                "index": 0,
                "is_finished": True,
                "text": f"å¤„ç†å¤±è´¥: {error_msg}",
                "tool_use": None,
                "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
            }
            yield error_chunk

    def _extract_text_from_sse_data(self, sse_data: dict) -> str:
        """
        ä»SSEæ•°æ®ä¸­æå–æ–‡æœ¬å†…å®¹
        
        Args:
            sse_data: SSEæ•°æ®å­—å…¸
            
        Returns:
            str: æå–çš„æ–‡æœ¬å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰æ–‡æœ¬å†…å®¹åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œå¦‚æœæ˜¯workflow_finishedåˆ™è¿”å›ç‰¹æ®Šæ ‡è®°
        """
        if not isinstance(sse_data, dict):
            return ""
        
        # å¤„ç†text_chunkäº‹ä»¶
        if sse_data.get("event") == "text_chunk":
            text_content = sse_data.get("data", {}).get("text", "")
            if text_content:
                return text_content
        
        # å¤„ç†workflow_finishedäº‹ä»¶
        elif sse_data.get("event") == "workflow_finished":
            # å·¥ä½œæµå®Œæˆï¼Œè¿”å›ç‰¹æ®Šæ ‡è®°
            return "__WORKFLOW_FINISHED__"
        
        # å¤„ç†å…¶ä»–äº‹ä»¶ç±»å‹ï¼ˆnode_started, node_finishedç­‰ï¼‰
        elif sse_data.get("event") in ["node_started", "node_finished", "workflow_started"]:
            # è¿™äº›äº‹ä»¶ä¸åŒ…å«æ–‡æœ¬å†…å®¹ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
            return ""
        
        # å¦‚æœä¸æ˜¯äº‹ä»¶æ ¼å¼ï¼Œå°è¯•ç›´æ¥æå–textæˆ–contentå­—æ®µ
        else:
            text_content = sse_data.get("text", "") or sse_data.get("content", "")
            if text_content:
                return text_content
        
        return ""

# åˆ›å»ºå®ä¾‹
my_custom_llm = MyCustomLLM()
